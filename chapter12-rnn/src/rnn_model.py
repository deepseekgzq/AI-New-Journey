"""
ç¬¬12ç« ï¼šRNNæƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°
ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±»é¡¹ç›®
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import re
import string
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class SentimentAnalyzer:
    """æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self, max_features=20000, max_length=200):
        """åˆå§‹åŒ–"""
        self.max_features = max_features  # è¯æ±‡è¡¨å¤§å°
        self.max_length = max_length      # åºåˆ—æœ€å¤§é•¿åº¦
        self.tokenizer = None
        self.model = None
        self.history = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
    def generate_sample_data(self, n_samples=10000):
        """ç”Ÿæˆæ¨¡æ‹Ÿç”µå½±è¯„è®ºæ•°æ®"""
        print("ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿç”µå½±è¯„è®ºæ•°æ®...")
        
        np.random.seed(42)
        
        # æ­£é¢è¯„è®ºçš„å¸¸ç”¨è¯æ±‡
        positive_words = [
            'excellent', 'amazing', 'wonderful', 'fantastic', 'great', 'awesome',
            'brilliant', 'outstanding', 'perfect', 'incredible', 'superb', 'magnificent',
            'love', 'beautiful', 'impressive', 'stunning', 'remarkable', 'exceptional',
            'good', 'nice', 'enjoy', 'happy', 'pleased', 'satisfied', 'delighted'
        ]
        
        # è´Ÿé¢è¯„è®ºçš„å¸¸ç”¨è¯æ±‡
        negative_words = [
            'terrible', 'awful', 'horrible', 'bad', 'worst', 'disgusting',
            'boring', 'stupid', 'waste', 'disappointing', 'pathetic', 'useless',
            'hate', 'annoying', 'frustrating', 'ridiculous', 'nonsense', 'garbage',
            'poor', 'weak', 'failed', 'mess', 'disaster', 'unbearable'
        ]
        
        # ä¸­æ€§è¯æ±‡
        neutral_words = [
            'movie', 'film', 'story', 'plot', 'character', 'actor', 'actress',
            'director', 'scene', 'dialogue', 'music', 'sound', 'visual', 'effect',
            'action', 'drama', 'comedy', 'thriller', 'romance', 'adventure',
            'watch', 'see', 'think', 'feel', 'experience', 'show', 'performance'
        ]
        
        reviews = []
        labels = []
        
        for i in range(n_samples):
            # éšæœºå†³å®šæƒ…æ„Ÿ
            sentiment = np.random.choice([0, 1])  # 0: è´Ÿé¢, 1: æ­£é¢
            
            if sentiment == 1:  # æ­£é¢è¯„è®º
                # é€‰æ‹©æ›´å¤šæ­£é¢è¯æ±‡
                chosen_positive = np.random.choice(positive_words, 
                                                 size=np.random.randint(3, 8), replace=True)
                chosen_neutral = np.random.choice(neutral_words, 
                                                size=np.random.randint(5, 12), replace=True)
                chosen_negative = np.random.choice(negative_words, 
                                                 size=np.random.randint(0, 2), replace=True)
                words = np.concatenate([chosen_positive, chosen_neutral, chosen_negative])
            else:  # è´Ÿé¢è¯„è®º
                # é€‰æ‹©æ›´å¤šè´Ÿé¢è¯æ±‡
                chosen_negative = np.random.choice(negative_words, 
                                                 size=np.random.randint(3, 8), replace=True)
                chosen_neutral = np.random.choice(neutral_words, 
                                                size=np.random.randint(5, 12), replace=True)
                chosen_positive = np.random.choice(positive_words, 
                                                  size=np.random.randint(0, 2), replace=True)
                words = np.concatenate([chosen_negative, chosen_neutral, chosen_positive])
            
            # éšæœºæ‰“ä¹±è¯åº
            np.random.shuffle(words)
            
            # æ„å»ºè¯„è®º
            review = ' '.join(words)
            
            reviews.append(review)
            labels.append(sentiment)
        
        # åˆ›å»ºDataFrame
        data = pd.DataFrame({
            'review': reviews,
            'sentiment': labels
        })
        
        print(f"âœ… ç”Ÿæˆäº† {len(data)} æ¡è¯„è®ºæ•°æ®")
        print(f"   æ­£é¢è¯„è®º: {sum(labels)} æ¡")
        print(f"   è´Ÿé¢è¯„è®º: {len(labels) - sum(labels)} æ¡")
        
        return data
    
    def load_data(self, file_path=None):
        """åŠ è½½ç”µå½±è¯„è®ºæ•°æ®"""
        print("ğŸ“‚ åŠ è½½ç”µå½±è¯„è®ºæ•°æ®...")
        
        if file_path and pd.io.common.file_exists(file_path):
            data = pd.read_csv(file_path)
        else:
            data = self.generate_sample_data()
        
        print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"æ­£è´Ÿæ ·æœ¬åˆ†å¸ƒ:")
        print(data['sentiment'].value_counts())
        
        return data
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = text.translate(str.maketrans('', '', string.punctuation))
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\\s+', ' ', text).strip()
        
        return text
    
    def explore_data(self, data):
        """æ•°æ®æ¢ç´¢åˆ†æ"""
        print("\\nğŸ” æ•°æ®æ¢ç´¢åˆ†æ...")
        
        # æ–‡æœ¬é•¿åº¦åˆ†æ
        data['review_length'] = data['review'].str.len()
        data['word_count'] = data['review'].str.split().str.len()
        
        # å¯è§†åŒ–åˆ†æ
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. æƒ…æ„Ÿåˆ†å¸ƒ
        sentiment_counts = data['sentiment'].value_counts()
        labels = ['è´Ÿé¢', 'æ­£é¢']
        axes[0, 0].pie(sentiment_counts.values, labels=labels, autopct='%1.1f%%',
                      colors=['lightcoral', 'lightblue'], startangle=90)
        axes[0, 0].set_title('æƒ…æ„Ÿåˆ†å¸ƒ', fontweight='bold')
        
        # 2. è¯„è®ºé•¿åº¦åˆ†å¸ƒ
        axes[0, 1].hist([data[data['sentiment']==0]['review_length'],
                        data[data['sentiment']==1]['review_length']],
                       bins=50, alpha=0.7, label=['è´Ÿé¢', 'æ­£é¢'],
                       color=['lightcoral', 'lightblue'])
        axes[0, 1].set_title('è¯„è®ºé•¿åº¦åˆ†å¸ƒ', fontweight='bold')
        axes[0, 1].set_xlabel('å­—ç¬¦æ•°')
        axes[0, 1].set_ylabel('é¢‘æ•°')
        axes[0, 1].legend()
        
        # 3. è¯æ•°åˆ†å¸ƒ
        axes[1, 0].hist([data[data['sentiment']==0]['word_count'],
                        data[data['sentiment']==1]['word_count']],
                       bins=30, alpha=0.7, label=['è´Ÿé¢', 'æ­£é¢'],
                       color=['lightcoral', 'lightblue'])
        axes[1, 0].set_title('è¯æ•°åˆ†å¸ƒ', fontweight='bold')
        axes[1, 0].set_xlabel('è¯æ•°')
        axes[1, 0].set_ylabel('é¢‘æ•°')
        axes[1, 0].legend()
        
        # 4. è¯é¢‘åˆ†æ
        all_words = []
        for review in data['review']:
            all_words.extend(review.split())
        
        word_freq = Counter(all_words)
        top_words = word_freq.most_common(20)
        
        words, freqs = zip(*top_words)
        axes[1, 1].barh(range(len(words)), freqs)
        axes[1, 1].set_yticks(range(len(words)))
        axes[1, 1].set_yticklabels(words)
        axes[1, 1].set_title('é«˜é¢‘è¯TOP20', fontweight='bold')
        axes[1, 1].set_xlabel('é¢‘æ•°')
        
        plt.tight_layout()
        plt.show()
        
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  â€¢ å¹³å‡è¯„è®ºé•¿åº¦: {data['review_length'].mean():.1f} å­—ç¬¦")
        print(f"  â€¢ å¹³å‡è¯æ•°: {data['word_count'].mean():.1f} è¯")
        print(f"  â€¢ è¯æ±‡æ€»æ•°: {len(set(all_words)):,}")
        print(f"  â€¢ æœ€é«˜é¢‘è¯: {top_words[0][0]} (å‡ºç°{top_words[0][1]}æ¬¡)")
    
    def prepare_data(self, data):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("\\nğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # æ–‡æœ¬é¢„å¤„ç†
        data['review_clean'] = data['review'].apply(self.preprocess_text)
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        texts = data['review_clean'].values
        labels = data['sentiment'].values
        
        # æ–‡æœ¬å‘é‡åŒ–
        self.tokenizer = Tokenizer(num_words=self.max_features, oov_token='<OOV>')
        self.tokenizer.fit_on_texts(texts)
        
        # è½¬æ¢ä¸ºåºåˆ—
        sequences = self.tokenizer.texts_to_sequences(texts)
        
        # å¡«å……åºåˆ—
        X = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.word_index)}")
        print(f"   åºåˆ—é•¿åº¦: {self.max_length}")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def build_model(self, embedding_dim=128, lstm_units=64):
        """æ„å»ºLSTMæ¨¡å‹"""
        print("\\nğŸ—ï¸ æ„å»ºLSTMæ¨¡å‹...")
        
        model = Sequential([
            # è¯åµŒå…¥å±‚
            Embedding(input_dim=self.max_features, 
                     output_dim=embedding_dim, 
                     input_length=self.max_length,
                     mask_zero=True),
            
            # åŒå‘LSTMå±‚
            Bidirectional(LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.3)),
            
            # å…¨è¿æ¥å±‚
            Dense(64, activation='relu'),
            Dropout(0.5),
            
            # è¾“å‡ºå±‚
            Dense(1, activation='sigmoid')
        ])
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        
        print("âœ… LSTMæ¨¡å‹æ„å»ºå®Œæˆ!")
        print(f"   æ€»å‚æ•°æ•°é‡: {model.count_params():,}")
        
        # æ˜¾ç¤ºæ¨¡å‹ç»“æ„
        print("\\nğŸ“‹ æ¨¡å‹æ¶æ„:")
        model.summary()
        
        return model
    
    def train_model(self, epochs=20, batch_size=32, validation_split=0.2):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\\nğŸ¤– è®­ç»ƒLSTMæ¨¡å‹ (epochs={epochs})...")
        
        # è®¾ç½®å›è°ƒå‡½æ•°
        callbacks = [
            EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        self.history = self.model.fit(
            self.X_train, self.y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        
        return self.history
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        print("\\nğŸ“ˆ å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹...")
        
        if self.history is None:
            print("âŒ æ²¡æœ‰è®­ç»ƒå†å²è®°å½•")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax1.plot(self.history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        ax1.plot(self.history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡', fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('å‡†ç¡®ç‡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æŸå¤±æ›²çº¿
        ax2.plot(self.history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax2.plot(self.history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
        ax2.set_title('æ¨¡å‹æŸå¤±', fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('æŸå¤±å€¼')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°æœ€ä½³æ€§èƒ½
        best_val_acc = max(self.history.history['val_accuracy'])
        best_epoch = self.history.history['val_accuracy'].index(best_val_acc) + 1
        print(f"ğŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (Epoch {best_epoch})")
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        
        # æµ‹è¯•é›†é¢„æµ‹
        y_pred_proba = self.model.predict(self.X_test, verbose=0)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(self.y_test, y_pred)
        
        print(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        print("\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        target_names = ['è´Ÿé¢', 'æ­£é¢']
        print(classification_report(self.y_test, y_pred, target_names=target_names))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=target_names, yticklabels=target_names)
        plt.title('æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.tight_layout()
        plt.show()
        
        return accuracy, y_pred, y_pred_proba
    
    def predict_sentiment(self, text):
        """é¢„æµ‹å•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ"""
        if isinstance(text, str):
            text = [text]
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_text = [self.preprocess_text(t) for t in text]
        
        # è½¬æ¢ä¸ºåºåˆ—
        sequences = self.tokenizer.texts_to_sequences(processed_text)
        padded = pad_sequences(sequences, maxlen=self.max_length, 
                              padding='post', truncating='post')
        
        # é¢„æµ‹
        predictions = self.model.predict(padded, verbose=0)
        
        results = []
        for i, pred in enumerate(predictions):
            sentiment = "æ­£é¢" if pred[0] > 0.5 else "è´Ÿé¢"
            confidence = pred[0] if pred[0] > 0.5 else 1 - pred[0]
            results.append({
                'text': text[i],
                'sentiment': sentiment,
                'confidence': confidence,
                'score': pred[0]
            })
        
        return results[0] if len(results) == 1 else results
    
    def analyze_word_importance(self, text, top_n=10):
        """åˆ†æè¯è¯­é‡è¦æ€§"""
        print(f"\\nğŸ” åˆ†æè¯è¯­é‡è¦æ€§: '{text[:50]}...'")
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_text = self.preprocess_text(text)
        words = processed_text.split()
        
        # è®¡ç®—æ¯ä¸ªè¯å¯¹é¢„æµ‹ç»“æœçš„å½±å“
        original_pred = self.predict_sentiment(text)['score']
        
        word_importance = []
        
        for i, word in enumerate(words):
            # ç§»é™¤è¿™ä¸ªè¯åçš„æ–‡æœ¬
            modified_words = words[:i] + words[i+1:]
            modified_text = ' '.join(modified_words)
            
            if modified_text.strip():  # ç¡®ä¿ä¸æ˜¯ç©ºæ–‡æœ¬
                modified_pred = self.predict_sentiment(modified_text)['score']
                importance = abs(original_pred - modified_pred)
                word_importance.append((word, importance))
        
        # æ’åºå¹¶è¿”å›æœ€é‡è¦çš„è¯
        word_importance.sort(key=lambda x: x[1], reverse=True)
        
        print(f"é‡è¦è¯è¯­ TOP{top_n}:")
        for i, (word, importance) in enumerate(word_importance[:top_n], 1):
            print(f"  {i}. {word}: {importance:.4f}")
        
        return word_importance[:top_n]
    
    def demonstrate_predictions(self):
        """æ¼”ç¤ºé¢„æµ‹åŠŸèƒ½"""
        print("\\nğŸ”® æƒ…æ„Ÿåˆ†ææ¼”ç¤º...")
        
        # ç¤ºä¾‹æ–‡æœ¬
        sample_texts = [
            "This movie is absolutely fantastic! I loved every minute of it.",
            "Terrible film, waste of time and money. Very disappointing.",
            "The movie was okay, nothing special but not bad either.",
            "Amazing performance by the actors, brilliant story and direction.",
            "Boring plot, terrible acting, one of the worst movies ever."
        ]
        
        results = []
        for text in sample_texts:
            result = self.predict_sentiment(text)
            results.append(result)
            print(f"æ–‡æœ¬: {text}")
            print(f"æƒ…æ„Ÿ: {result['sentiment']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            print("-" * 50)
        
        return results
    
    def save_model(self, filepath='sentiment_lstm.h5'):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is not None:
            self.model.save(filepath)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")
        else:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
    
    def load_model(self, filepath='sentiment_lstm.h5'):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.model = tf.keras.models.load_model(filepath)
            print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            return self.model
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def run_complete_project(self):
        """è¿è¡Œå®Œæ•´çš„æƒ…æ„Ÿåˆ†æé¡¹ç›®"""
        print("ğŸ’¬ å¼€å§‹ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æé¡¹ç›®...")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        data = self.load_data()
        
        # 2. æ•°æ®æ¢ç´¢
        self.explore_data(data)
        
        # 3. å‡†å¤‡æ•°æ®
        self.prepare_data(data)
        
        # 4. æ„å»ºæ¨¡å‹
        self.build_model()
        
        # 5. è®­ç»ƒæ¨¡å‹
        self.train_model(epochs=10)  # å‡å°‘epochsç”¨äºæ¼”ç¤º
        
        # 6. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        self.plot_training_history()
        
        # 7. è¯„ä¼°æ¨¡å‹
        accuracy, y_pred, y_pred_proba = self.evaluate_model()
        
        # 8. æ¼”ç¤ºé¢„æµ‹
        self.demonstrate_predictions()
        
        # 9. è¯è¯­é‡è¦æ€§åˆ†æ
        sample_text = "This movie is absolutely amazing and fantastic!"
        self.analyze_word_importance(sample_text)
        
        # 10. ä¿å­˜æ¨¡å‹
        self.save_model()
        
        print("\\nğŸ¯ é¡¹ç›®æ€»ç»“:")
        print(f"  â€¢ æ•°æ®é›†: {len(data)} æ¡ç”µå½±è¯„è®º")
        print(f"  â€¢ æ¨¡å‹: åŒå‘LSTMç½‘ç»œ")
        print(f"  â€¢ æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  â€¢ è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.word_index)}")
        print("\\nğŸ‰ ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æé¡¹ç›®å®Œæˆ!")
        
        return self

def demonstrate_rnn_concepts():
    """æ¼”ç¤ºRNNæ ¸å¿ƒæ¦‚å¿µ"""
    print("\\n" + "="*60)
    print("ğŸ§  RNNæ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º")
    print("="*60)
    
    print("\\n1ï¸âƒ£ åºåˆ—æ•°æ®çš„ç‰¹ç‚¹:")
    print("æ–‡æœ¬: 'I love this movie'")
    print("åºåˆ—: [I] -> [love] -> [this] -> [movie]")
    print("ç‰¹ç‚¹: æ¯ä¸ªè¯çš„ç†è§£ä¾èµ–äºå‰é¢çš„ä¸Šä¸‹æ–‡")
    
    print("\\n2ï¸âƒ£ RNN vs æ™®é€šç¥ç»ç½‘ç»œ:")
    print("æ™®é€šNN: è¾“å…¥ -> éšè—å±‚ -> è¾“å‡º (æ— è®°å¿†)")
    print("RNN: è¾“å…¥ + å†å²çŠ¶æ€ -> éšè—å±‚ -> è¾“å‡º (æœ‰è®°å¿†)")
    
    print("\\n3ï¸âƒ£ LSTMçš„ä¼˜åŠ¿:")
    print("â€¢ è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
    print("â€¢ èƒ½è®°ä½é•¿æœŸä¾èµ–å…³ç³»")
    print("â€¢ é€šè¿‡é—¨æ§æœºåˆ¶é€‰æ‹©æ€§è®°å¿†")
    
    # ç®€å•çš„åºåˆ—é¢„æµ‹æ¼”ç¤º
    print("\\n4ï¸âƒ£ åºåˆ—é¢„æµ‹æ¼”ç¤º:")
    
    # ç”Ÿæˆç®€å•çš„æ•°å­¦åºåˆ—
    sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    print(f"åŸå§‹åºåˆ—: {sequence}")
    
    # æ¨¡æ‹ŸRNNé¢„æµ‹ä¸‹ä¸€ä¸ªæ•°å­—
    def simple_rnn_predict(seq, window_size=3):
        if len(seq) < window_size:
            return None
        
        # ç®€å•çš„çº¿æ€§é¢„æµ‹ï¼ˆå®é™…RNNä¼šå­¦ä¹ æ›´å¤æ‚çš„æ¨¡å¼ï¼‰
        recent = seq[-window_size:]
        diff = recent[-1] - recent[-2] if len(recent) > 1 else 1
        predicted = recent[-1] + diff
        return predicted
    
    predicted = simple_rnn_predict(sequence)
    print(f"é¢„æµ‹ä¸‹ä¸€ä¸ªæ•°å­—: {predicted}")
    print(f"å®é™…åº”è¯¥æ˜¯: 11")
    
    print("\\nâœ… RNNèƒ½å¤Ÿå­¦ä¹ åºåˆ—ä¸­çš„æ¨¡å¼å¹¶è¿›è¡Œé¢„æµ‹ï¼")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ’¬ ç¬¬12ç« ï¼šRNNä¸æƒ…æ„Ÿåˆ†æ")
    print("=" * 60)
    
    # 1. æ¦‚å¿µæ¼”ç¤º
    demonstrate_rnn_concepts()
    
    # 2. è¿è¡Œå®Œæ•´é¡¹ç›®
    analyzer = SentimentAnalyzer()
    analyzer.run_complete_project()
    
    print("\\nğŸ“ å­¦ä¹ æ€»ç»“:")
    print("  â€¢ ç†è§£äº†RNNå’ŒLSTMçš„å·¥ä½œåŸç†")
    print("  â€¢ æŒæ¡äº†æ–‡æœ¬é¢„å¤„ç†å’Œå‘é‡åŒ–æŠ€æœ¯") 
    print("  â€¢ å®Œæˆäº†ç«¯åˆ°ç«¯çš„æƒ…æ„Ÿåˆ†æé¡¹ç›®")
    print("  â€¢ å­¦ä¼šäº†åºåˆ—å»ºæ¨¡å’Œè‡ªç„¶è¯­è¨€å¤„ç†")
    print("  â€¢ å…·å¤‡äº†å¤„ç†æ–‡æœ¬æ•°æ®çš„AIèƒ½åŠ›")
    
    return analyzer

if __name__ == "__main__":
    analyzer = main()
