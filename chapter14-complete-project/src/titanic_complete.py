"""
ç¬¬14ç« ï¼šæ³°å¦å°¼å…‹å·ç”Ÿè¿˜é¢„æµ‹å®Œæ•´é¡¹ç›®
ä»æ•°æ®åˆ°æ¨¡å‹çš„ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ é¡¹ç›®
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class TitanicSurvivalPredictor:
    """æ³°å¦å°¼å…‹å·ç”Ÿè¿˜é¢„æµ‹å®Œæ•´é¡¹ç›®"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.train_data = None
        self.test_data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.models = {}
        self.best_model = None
        self.feature_names = None
        self.scaler = StandardScaler()
        
    def load_data(self):
        """åŠ è½½æ³°å¦å°¼å…‹å·æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ³°å¦å°¼å…‹å·æ•°æ®é›†...")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ³°å¦å°¼å…‹å·æ•°æ®
        self.train_data = self._generate_titanic_data(n_samples=891, is_train=True)
        self.test_data = self._generate_titanic_data(n_samples=418, is_train=False)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
        print(f"   - è®­ç»ƒé›†: {len(self.train_data)} æ¡è®°å½•")
        print(f"   - æµ‹è¯•é›†: {len(self.test_data)} æ¡è®°å½•")
        
        return self.train_data, self.test_data
    
    def _generate_titanic_data(self, n_samples=891, is_train=True):
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„æ³°å¦å°¼å…‹å·æ•°æ®"""
        np.random.seed(42 if is_train else 24)
        
        # ç”ŸæˆåŸºç¡€ç‰¹å¾
        data = {
            'PassengerId': range(1 if is_train else 892, n_samples + (1 if is_train else 892)),
            'Pclass': np.random.choice([1, 2, 3], n_samples, p=[0.24, 0.21, 0.55]),
            'Sex': np.random.choice(['male', 'female'], n_samples, p=[0.65, 0.35]),
            'Age': np.random.normal(29.7, 14.5, n_samples),
            'SibSp': np.random.choice(range(9), n_samples, p=[0.68, 0.23, 0.06, 0.02, 0.005, 0.003, 0.001, 0.001, 0.001]),
            'Parch': np.random.choice(range(7), n_samples, p=[0.76, 0.13, 0.08, 0.005, 0.004, 0.001, 0.001]),
            'Fare': np.random.lognormal(2.5, 1.2, n_samples),
            'Embarked': np.random.choice(['S', 'C', 'Q'], n_samples, p=[0.72, 0.19, 0.09])
        }
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(data)
        
        # å¤„ç†å¹´é¾„ï¼ˆå¼•å…¥ä¸€äº›ç¼ºå¤±å€¼ï¼‰
        missing_age_mask = np.random.random(n_samples) < 0.2
        df.loc[missing_age_mask, 'Age'] = np.nan
        
        # ç¡®ä¿å¹´é¾„åœ¨åˆç†èŒƒå›´å†…
        df['Age'] = np.clip(df['Age'], 0.42, 80)
        
        # ç¡®ä¿ç¥¨ä»·åœ¨åˆç†èŒƒå›´å†…
        df['Fare'] = np.clip(df['Fare'], 0, 512)
        
        # æ·»åŠ å§“åï¼ˆç®€åŒ–ç‰ˆï¼‰
        titles = ['Mr.', 'Mrs.', 'Miss.', 'Master.', 'Dr.', 'Rev.']
        df['Name'] = [f"{np.random.choice(titles)} {chr(65+i%26)}{chr(65+(i*7)%26)}" for i in range(n_samples)]
        
        # æ·»åŠ èˆ¹ç¥¨å·ç ï¼ˆç®€åŒ–ç‰ˆï¼‰
        df['Ticket'] = [f"T{1000+i}" for i in range(n_samples)]
        
        # æ·»åŠ èˆ¹èˆ±å·ç ï¼ˆéƒ¨åˆ†ç¼ºå¤±ï¼‰
        cabin_mask = np.random.random(n_samples) < 0.23
        df['Cabin'] = [f"{np.random.choice(['A','B','C','D','E','F','G'])}{np.random.randint(1,200)}" 
                      if cabin_mask[i] else np.nan for i in range(n_samples)]
        
        # ç”Ÿæˆç”Ÿè¿˜æ ‡ç­¾ï¼ˆä»…è®­ç»ƒé›†ï¼‰
        if is_train:
            # åŸºäºç‰¹å¾ç”Ÿæˆè¾ƒä¸ºçœŸå®çš„ç”Ÿè¿˜æ¦‚ç‡
            survival_prob = 0.5  # åŸºç¡€æ¦‚ç‡
            
            # æ€§åˆ«å½±å“ï¼ˆå¥³æ€§ç”Ÿè¿˜ç‡æ›´é«˜ï¼‰
            survival_prob += np.where(df['Sex'] == 'female', 0.3, -0.2)
            
            # å¹´é¾„å½±å“ï¼ˆå„¿ç«¥ç”Ÿè¿˜ç‡æ›´é«˜ï¼‰
            survival_prob += np.where(df['Age'] < 16, 0.2, 0)
            survival_prob += np.where(df['Age'] > 60, -0.1, 0)
            
            # èˆ¹ç¥¨ç­‰çº§å½±å“ï¼ˆé«˜ç­‰çº§ç”Ÿè¿˜ç‡æ›´é«˜ï¼‰
            pclass_effect = {1: 0.25, 2: 0.05, 3: -0.15}
            survival_prob += df['Pclass'].map(pclass_effect)
            
            # å®¶åº­è§„æ¨¡å½±å“
            family_size = df['SibSp'] + df['Parch']
            survival_prob += np.where((family_size >= 1) & (family_size <= 3), 0.1, -0.05)
            
            # ç¥¨ä»·å½±å“
            survival_prob += np.where(df['Fare'] > df['Fare'].median(), 0.1, -0.05)
            
            # ç™»èˆ¹æ¸¯å£å½±å“
            embarked_effect = {'C': 0.1, 'Q': -0.05, 'S': 0}
            survival_prob += df['Embarked'].map(embarked_effect)
            
            # æ·»åŠ ä¸€äº›éšæœºæ€§
            survival_prob += np.random.normal(0, 0.1, n_samples)
            
            # ç¡®ä¿æ¦‚ç‡åœ¨0-1èŒƒå›´å†…
            survival_prob = np.clip(survival_prob, 0, 1)
            
            # ç”ŸæˆäºŒåˆ†ç±»æ ‡ç­¾
            df['Survived'] = np.random.binomial(1, survival_prob)
        
        return df
    
    def exploratory_data_analysis(self):
        """æ¢ç´¢æ€§æ•°æ®åˆ†æ"""
        print("\\nğŸ” æ¢ç´¢æ€§æ•°æ®åˆ†æ...")
        
        # åŸºæœ¬ä¿¡æ¯
        print("\\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.train_data.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.test_data.shape}")
        
        print("\\nğŸ“‹ ç‰¹å¾ä¿¡æ¯:")
        print(self.train_data.info())
        
        print("\\nğŸ“ˆ ç”Ÿè¿˜ç‡ç»Ÿè®¡:")
        survival_rate = self.train_data['Survived'].mean()
        print(f"æ€»ä½“ç”Ÿè¿˜ç‡: {survival_rate:.3f}")
        
        # å¯è§†åŒ–åˆ†æ
        fig, axes = plt.subplots(3, 3, figsize=(20, 15))
        
        # 1. ç”Ÿè¿˜ç‡åˆ†å¸ƒ
        survival_counts = self.train_data['Survived'].value_counts()
        axes[0, 0].pie(survival_counts.values, labels=['æœªç”Ÿè¿˜', 'ç”Ÿè¿˜'], autopct='%1.1f%%', 
                      colors=['lightcoral', 'lightblue'])
        axes[0, 0].set_title('ç”Ÿè¿˜ç‡åˆ†å¸ƒ', fontweight='bold')
        
        # 2. æ€§åˆ«ä¸ç”Ÿè¿˜ç‡
        sex_survival = pd.crosstab(self.train_data['Sex'], self.train_data['Survived'], normalize='index')
        sex_survival.plot(kind='bar', ax=axes[0, 1], color=['lightcoral', 'lightblue'])
        axes[0, 1].set_title('æ€§åˆ«ä¸ç”Ÿè¿˜ç‡', fontweight='bold')
        axes[0, 1].set_xlabel('æ€§åˆ«')
        axes[0, 1].set_ylabel('ç”Ÿè¿˜ç‡')
        axes[0, 1].legend(['æœªç”Ÿè¿˜', 'ç”Ÿè¿˜'])
        axes[0, 1].tick_params(axis='x', rotation=0)
        
        # 3. èˆ¹ç¥¨ç­‰çº§ä¸ç”Ÿè¿˜ç‡
        pclass_survival = pd.crosstab(self.train_data['Pclass'], self.train_data['Survived'], normalize='index')
        pclass_survival.plot(kind='bar', ax=axes[0, 2], color=['lightcoral', 'lightblue'])
        axes[0, 2].set_title('èˆ¹ç¥¨ç­‰çº§ä¸ç”Ÿè¿˜ç‡', fontweight='bold')
        axes[0, 2].set_xlabel('èˆ¹ç¥¨ç­‰çº§')
        axes[0, 2].set_ylabel('ç”Ÿè¿˜ç‡')
        axes[0, 2].legend(['æœªç”Ÿè¿˜', 'ç”Ÿè¿˜'])
        axes[0, 2].tick_params(axis='x', rotation=0)
        
        # 4. å¹´é¾„åˆ†å¸ƒ
        axes[1, 0].hist([self.train_data[self.train_data['Survived']==0]['Age'].dropna(),
                        self.train_data[self.train_data['Survived']==1]['Age'].dropna()],
                       bins=30, alpha=0.7, label=['æœªç”Ÿè¿˜', 'ç”Ÿè¿˜'], color=['lightcoral', 'lightblue'])
        axes[1, 0].set_title('å¹´é¾„åˆ†å¸ƒ', fontweight='bold')
        axes[1, 0].set_xlabel('å¹´é¾„')
        axes[1, 0].set_ylabel('äººæ•°')
        axes[1, 0].legend()
        
        # 5. ç¥¨ä»·åˆ†å¸ƒ
        axes[1, 1].hist([self.train_data[self.train_data['Survived']==0]['Fare'],
                        self.train_data[self.train_data['Survived']==1]['Fare']],
                       bins=50, alpha=0.7, label=['æœªç”Ÿè¿˜', 'ç”Ÿè¿˜'], color=['lightcoral', 'lightblue'])
        axes[1, 1].set_title('ç¥¨ä»·åˆ†å¸ƒ', fontweight='bold')
        axes[1, 1].set_xlabel('ç¥¨ä»·')
        axes[1, 1].set_ylabel('äººæ•°')
        axes[1, 1].legend()
        axes[1, 1].set_xlim(0, 200)  # é™åˆ¶æ˜¾ç¤ºèŒƒå›´
        
        # 6. å®¶åº­è§„æ¨¡ä¸ç”Ÿè¿˜ç‡
        self.train_data['FamilySize'] = self.train_data['SibSp'] + self.train_data['Parch'] + 1
        family_survival = self.train_data.groupby('FamilySize')['Survived'].mean()
        axes[1, 2].bar(family_survival.index, family_survival.values, color='skyblue', alpha=0.7)
        axes[1, 2].set_title('å®¶åº­è§„æ¨¡ä¸ç”Ÿè¿˜ç‡', fontweight='bold')
        axes[1, 2].set_xlabel('å®¶åº­è§„æ¨¡')
        axes[1, 2].set_ylabel('ç”Ÿè¿˜ç‡')
        
        # 7. ç™»èˆ¹æ¸¯å£ä¸ç”Ÿè¿˜ç‡
        embarked_survival = pd.crosstab(self.train_data['Embarked'], self.train_data['Survived'], normalize='index')
        embarked_survival.plot(kind='bar', ax=axes[2, 0], color=['lightcoral', 'lightblue'])
        axes[2, 0].set_title('ç™»èˆ¹æ¸¯å£ä¸ç”Ÿè¿˜ç‡', fontweight='bold')
        axes[2, 0].set_xlabel('ç™»èˆ¹æ¸¯å£')
        axes[2, 0].set_ylabel('ç”Ÿè¿˜ç‡')
        axes[2, 0].legend(['æœªç”Ÿè¿˜', 'ç”Ÿè¿˜'])
        axes[2, 0].tick_params(axis='x', rotation=0)
        
        # 8. ç›¸å…³æ€§çƒ­åŠ›å›¾
        numeric_features = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize']
        correlation_matrix = self.train_data[numeric_features].corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[2, 1])
        axes[2, 1].set_title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾', fontweight='bold')
        
        # 9. ç¼ºå¤±å€¼åˆ†æ
        missing_data = self.train_data.isnull().sum()
        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
        if len(missing_data) > 0:
            axes[2, 2].bar(range(len(missing_data)), missing_data.values, color='orange', alpha=0.7)
            axes[2, 2].set_title('ç¼ºå¤±å€¼ç»Ÿè®¡', fontweight='bold')
            axes[2, 2].set_xlabel('ç‰¹å¾')
            axes[2, 2].set_ylabel('ç¼ºå¤±å€¼æ•°é‡')
            axes[2, 2].set_xticks(range(len(missing_data)))
            axes[2, 2].set_xticklabels(missing_data.index, rotation=45)
        else:
            axes[2, 2].text(0.5, 0.5, 'æ— ç¼ºå¤±å€¼', ha='center', va='center', transform=axes[2, 2].transAxes)
            axes[2, 2].set_title('ç¼ºå¤±å€¼ç»Ÿè®¡', fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        print("\\nğŸ’¡ æ•°æ®æ´å¯Ÿ:")
        print(f"  â€¢ å¥³æ€§ç”Ÿè¿˜ç‡æ˜¾è‘—é«˜äºç”·æ€§")
        print(f"  â€¢ é«˜ç­‰çº§èˆ¹ç¥¨ä¹˜å®¢ç”Ÿè¿˜ç‡æ›´é«˜") 
        print(f"  â€¢ å¹´é¾„å’Œå®¶åº­è§„æ¨¡å¯¹ç”Ÿè¿˜ç‡æœ‰å½±å“")
        print(f"  â€¢ ç¥¨ä»·ä¸ç”Ÿè¿˜ç‡å‘ˆæ­£ç›¸å…³")
    
    def feature_engineering(self):
        """ç‰¹å¾å·¥ç¨‹"""
        print("\\nğŸ”§ ç‰¹å¾å·¥ç¨‹...")
        
        # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œç»Ÿä¸€å¤„ç†
        all_data = pd.concat([self.train_data, self.test_data], ignore_index=True)
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        print("   å¤„ç†ç¼ºå¤±å€¼...")
        
        # å¹´é¾„ï¼šç”¨ä¸­ä½æ•°å¡«å……
        all_data['Age'].fillna(all_data['Age'].median(), inplace=True)
        
        # ç™»èˆ¹æ¸¯å£ï¼šç”¨ä¼—æ•°å¡«å……
        all_data['Embarked'].fillna(all_data['Embarked'].mode()[0], inplace=True)
        
        # ç¥¨ä»·ï¼šç”¨ä¸­ä½æ•°å¡«å……
        all_data['Fare'].fillna(all_data['Fare'].median(), inplace=True)
        
        # 2. åˆ›å»ºæ–°ç‰¹å¾
        print("   åˆ›å»ºæ–°ç‰¹å¾...")
        
        # å®¶åº­è§„æ¨¡
        all_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1
        
        # æ˜¯å¦ç‹¬è‡ªä¸€äºº
        all_data['IsAlone'] = (all_data['FamilySize'] == 1).astype(int)
        
        # å¹´é¾„åˆ†ç»„
        all_data['AgeGroup'] = pd.cut(all_data['Age'], bins=[0, 12, 18, 35, 60, 100], 
                                     labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
        
        # ç¥¨ä»·åˆ†ç»„
        all_data['FareGroup'] = pd.qcut(all_data['Fare'], q=4, labels=['Low', 'Medium', 'High', 'VeryHigh'])
        
        # ä»å§“åä¸­æå–ç§°è°“
        all_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)
        all_data['Title'] = all_data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\
                                                      'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
        all_data['Title'] = all_data['Title'].replace('Mlle', 'Miss')
        all_data['Title'] = all_data['Title'].replace('Ms', 'Miss')
        all_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')
        
        # èˆ¹èˆ±ç­‰çº§
        all_data['HasCabin'] = (~all_data['Cabin'].isnull()).astype(int)
        
        # 3. ç¼–ç åˆ†ç±»ç‰¹å¾
        print("   ç¼–ç åˆ†ç±»ç‰¹å¾...")
        
        # æ€§åˆ«ç¼–ç 
        le_sex = LabelEncoder()
        all_data['Sex_encoded'] = le_sex.fit_transform(all_data['Sex'])
        
        # ç™»èˆ¹æ¸¯å£ç¼–ç 
        le_embarked = LabelEncoder()
        all_data['Embarked_encoded'] = le_embarked.fit_transform(all_data['Embarked'])
        
        # ç§°è°“ç¼–ç 
        le_title = LabelEncoder()
        all_data['Title_encoded'] = le_title.fit_transform(all_data['Title'])
        
        # å¹´é¾„ç»„ç¼–ç 
        le_age_group = LabelEncoder()
        all_data['AgeGroup_encoded'] = le_age_group.fit_transform(all_data['AgeGroup'])
        
        # ç¥¨ä»·ç»„ç¼–ç 
        le_fare_group = LabelEncoder()
        all_data['FareGroup_encoded'] = le_fare_group.fit_transform(all_data['FareGroup'])
        
        # 4. é€‰æ‹©æœ€ç»ˆç‰¹å¾
        feature_columns = [
            'Pclass', 'Sex_encoded', 'Age', 'SibSp', 'Parch', 'Fare',
            'Embarked_encoded', 'FamilySize', 'IsAlone', 'HasCabin',
            'Title_encoded', 'AgeGroup_encoded', 'FareGroup_encoded'
        ]
        
        # åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_size = len(self.train_data)
        train_processed = all_data[:train_size]
        test_processed = all_data[train_size:]
        
        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = train_processed[feature_columns]
        y = train_processed['Survived']
        X_submission = test_processed[feature_columns]
        
        self.feature_names = feature_columns
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        print(f"   - æœ€ç»ˆç‰¹å¾æ•°é‡: {len(feature_columns)}")
        print(f"   - ç‰¹å¾åˆ—è¡¨: {feature_columns}")
        
        return X, y, X_submission
    
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("\\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # ç‰¹å¾å·¥ç¨‹
        X, y, X_submission = self.feature_engineering()
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   - è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
        print(f"   - éªŒè¯é›†: {len(self.X_test)} æ ·æœ¬")
        print(f"   - ç‰¹å¾æ•°é‡: {self.X_train.shape[1]}")
        
        return self.X_train_scaled, self.X_test_scaled, self.y_train, self.y_test
    
    def train_models(self):
        """è®­ç»ƒå¤šç§æ¨¡å‹"""
        print("\\nğŸ¤– è®­ç»ƒå¤šç§æ¨¡å‹...")
        
        # å®šä¹‰æ¨¡å‹
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),
            'SVM': SVC(random_state=42, probability=True)
        }
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
        results = {}
        
        for name, model in models.items():
            print(f"   è®­ç»ƒ {name}...")
            
            # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°
            cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5, scoring='accuracy')
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(self.X_train_scaled, self.y_train)
            
            # éªŒè¯é›†é¢„æµ‹
            y_pred = model.predict(self.X_test_scaled)
            y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1]
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(self.y_test, y_pred)
            auc = roc_auc_score(self.y_test, y_pred_proba)
            
            results[name] = {
                'model': model,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'val_accuracy': accuracy,
                'val_auc': auc,
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba
            }
            
            print(f"     CVå‡†ç¡®ç‡: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
            print(f"     éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"     éªŒè¯AUC: {auc:.4f}")
        
        self.models = results
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_name = max(results.keys(), key=lambda x: results[x]['val_auc'])
        self.best_model = results[best_model_name]['model']
        
        print(f"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"   éªŒè¯AUC: {results[best_model_name]['val_auc']:.4f}")
        
        return results
    
    def evaluate_models(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        
        if not self.models:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
        
        # åˆ›å»ºè¯„ä¼°å¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
        model_names = list(self.models.keys())
        cv_scores = [self.models[name]['cv_mean'] for name in model_names]
        val_scores = [self.models[name]['val_accuracy'] for name in model_names]
        auc_scores = [self.models[name]['val_auc'] for name in model_names]
        
        x = np.arange(len(model_names))
        width = 0.25
        
        axes[0, 0].bar(x - width, cv_scores, width, label='CVå‡†ç¡®ç‡', alpha=0.8)
        axes[0, 0].bar(x, val_scores, width, label='éªŒè¯å‡†ç¡®ç‡', alpha=0.8)
        axes[0, 0].bar(x + width, auc_scores, width, label='éªŒè¯AUC', alpha=0.8)
        
        axes[0, 0].set_xlabel('æ¨¡å‹')
        axes[0, 0].set_ylabel('åˆ†æ•°')
        axes[0, 0].set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontweight='bold')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(model_names, rotation=45)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ROCæ›²çº¿
        for name in model_names:
            y_pred_proba = self.models[name]['y_pred_proba']
            fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)
            auc = self.models[name]['val_auc']
            axes[0, 1].plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')
        
        axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)
        axes[0, 1].set_xlabel('å‡æ­£ç‡')
        axes[0, 1].set_ylabel('çœŸæ­£ç‡')
        axes[0, 1].set_title('ROCæ›²çº¿', fontweight='bold')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æœ€ä½³æ¨¡å‹çš„æ··æ·†çŸ©é˜µ
        best_model_name = max(self.models.keys(), key=lambda x: self.models[x]['val_auc'])
        best_y_pred = self.models[best_model_name]['y_pred']
        cm = confusion_matrix(self.y_test, best_y_pred)
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
        axes[1, 0].set_title(f'{best_model_name} æ··æ·†çŸ©é˜µ', fontweight='bold')
        axes[1, 0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[1, 0].set_ylabel('çœŸå®æ ‡ç­¾')
        
        # 4. ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
            feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importances
            }).sort_values('importance', ascending=True)
            
            axes[1, 1].barh(range(len(feature_importance)), feature_importance['importance'])
            axes[1, 1].set_yticks(range(len(feature_importance)))
            axes[1, 1].set_yticklabels(feature_importance['feature'])
            axes[1, 1].set_xlabel('é‡è¦æ€§')
            axes[1, 1].set_title('ç‰¹å¾é‡è¦æ€§', fontweight='bold')
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'è¯¥æ¨¡å‹ä¸æ”¯æŒ\\nç‰¹å¾é‡è¦æ€§åˆ†æ', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
            axes[1, 1].set_title('ç‰¹å¾é‡è¦æ€§', fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
        print(f"\\nğŸ“‹ æœ€ä½³æ¨¡å‹ ({best_model_name}) è¯¦ç»†æŠ¥å‘Š:")
        print(classification_report(self.y_test, best_y_pred, 
                                  target_names=['æœªç”Ÿè¿˜', 'ç”Ÿè¿˜']))
    
    def hyperparameter_tuning(self):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print("\\nâš™ï¸ è¶…å‚æ•°è°ƒä¼˜...")
        
        # ä¸ºæœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
        best_model_name = max(self.models.keys(), key=lambda x: self.models[x]['val_auc'])
        
        if 'Random Forest' in best_model_name:
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [3, 5, 7, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            base_model = RandomForestClassifier(random_state=42)
            
        elif 'Gradient Boosting' in best_model_name:
            param_grid = {
                'n_estimators': [100, 200],
                'learning_rate': [0.05, 0.1, 0.15],
                'max_depth': [3, 4, 5]
            }
            base_model = GradientBoostingClassifier(random_state=42)
            
        else:
            print(f"   {best_model_name} ä½¿ç”¨é»˜è®¤å‚æ•°")
            return self.best_model
        
        print(f"   ä¸º {best_model_name} è¿›è¡Œç½‘æ ¼æœç´¢...")
        
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            base_model, param_grid, cv=5, scoring='roc_auc', 
            n_jobs=-1, verbose=1
        )
        
        grid_search.fit(self.X_train_scaled, self.y_train)
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        self.best_model = grid_search.best_estimator_
        
        print(f"âœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆ!")
        print(f"   æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"   æœ€ä½³CVåˆ†æ•°: {grid_search.best_score_:.4f}")
        
        return self.best_model
    
    def generate_predictions(self, X_submission):
        """ç”Ÿæˆæäº¤é¢„æµ‹"""
        print("\\nğŸ”® ç”Ÿæˆæäº¤é¢„æµ‹...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®
        X_submission_scaled = self.scaler.transform(X_submission)
        
        # é¢„æµ‹
        predictions = self.best_model.predict(X_submission_scaled)
        probabilities = self.best_model.predict_proba(X_submission_scaled)[:, 1]
        
        print(f"âœ… é¢„æµ‹å®Œæˆ!")
        print(f"   é¢„æµ‹ç”Ÿè¿˜ç‡: {predictions.mean():.3f}")
        
        return predictions, probabilities
    
    def create_project_report(self):
        """åˆ›å»ºé¡¹ç›®æŠ¥å‘Š"""
        print("\\nğŸ“ ç”Ÿæˆé¡¹ç›®æŠ¥å‘Š...")
        
        best_model_name = max(self.models.keys(), key=lambda x: self.models[x]['val_auc'])
        best_result = self.models[best_model_name]
        
        report = f"""
# æ³°å¦å°¼å…‹å·ç”Ÿè¿˜é¢„æµ‹é¡¹ç›®æŠ¥å‘Š

## é¡¹ç›®æ¦‚è¿°
- **ç›®æ ‡**: é¢„æµ‹æ³°å¦å°¼å…‹å·ä¹˜å®¢ç”Ÿè¿˜æƒ…å†µ
- **æ•°æ®é›†**: æ³°å¦å°¼å…‹å·ä¹˜å®¢ä¿¡æ¯
- **é—®é¢˜ç±»å‹**: äºŒåˆ†ç±»é—®é¢˜

## æ•°æ®åˆ†æç»“æœ
- **è®­ç»ƒé›†è§„æ¨¡**: {len(self.train_data)} æ¡è®°å½•
- **æµ‹è¯•é›†è§„æ¨¡**: {len(self.test_data)} æ¡è®°å½•
- **ç‰¹å¾æ•°é‡**: {len(self.feature_names)}
- **æ€»ä½“ç”Ÿè¿˜ç‡**: {self.train_data['Survived'].mean():.3f}

## å…³é”®å‘ç°
1. **æ€§åˆ«å½±å“**: å¥³æ€§ç”Ÿè¿˜ç‡æ˜¾è‘—é«˜äºç”·æ€§
2. **ç¤¾ä¼šåœ°ä½**: é«˜ç­‰çº§èˆ¹ç¥¨ä¹˜å®¢ç”Ÿè¿˜ç‡æ›´é«˜
3. **å¹´é¾„å› ç´ **: å„¿ç«¥ç”Ÿè¿˜ç‡ç›¸å¯¹è¾ƒé«˜
4. **å®¶åº­è§„æ¨¡**: ä¸­ç­‰è§„æ¨¡å®¶åº­ç”Ÿè¿˜ç‡æœ€ä½³

## æ¨¡å‹æ€§èƒ½
- **æœ€ä½³æ¨¡å‹**: {best_model_name}
- **äº¤å‰éªŒè¯å‡†ç¡®ç‡**: {best_result['cv_mean']:.4f} (Â±{best_result['cv_std']:.4f})
- **éªŒè¯é›†å‡†ç¡®ç‡**: {best_result['val_accuracy']:.4f}
- **éªŒè¯é›†AUC**: {best_result['val_auc']:.4f}

## ç‰¹å¾å·¥ç¨‹
- å¤„ç†ç¼ºå¤±å€¼ï¼ˆå¹´é¾„ã€ç™»èˆ¹æ¸¯å£ç­‰ï¼‰
- åˆ›å»ºæ–°ç‰¹å¾ï¼ˆå®¶åº­è§„æ¨¡ã€æ˜¯å¦ç‹¬è‡ªä¸€äººç­‰ï¼‰
- ä»å§“åæå–ç§°è°“ä¿¡æ¯
- å¯¹åˆ†ç±»å˜é‡è¿›è¡Œç¼–ç 

## æ¨¡å‹å¯¹æ¯”
"""
        
        for name, result in self.models.items():
            report += f"- **{name}**: CV={result['cv_mean']:.4f}, Val={result['val_accuracy']:.4f}, AUC={result['val_auc']:.4f}\\n"
        
        report += f"""
## ç»“è®ºä¸å»ºè®®
1. {best_model_name} åœ¨æ‰€æœ‰æµ‹è¯•æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³
2. æ€§åˆ«æ˜¯æœ€é‡è¦çš„é¢„æµ‹å› å­
3. ç¤¾ä¼šç»æµåœ°ä½å¯¹ç”Ÿè¿˜ç‡æœ‰æ˜¾è‘—å½±å“
4. æ¨¡å‹å¯ç”¨äºç±»ä¼¼å†å²äº‹ä»¶åˆ†æ

## æŠ€æœ¯æ ˆ
- **æ•°æ®å¤„ç†**: Pandas, NumPy
- **å¯è§†åŒ–**: Matplotlib, Seaborn  
- **æœºå™¨å­¦ä¹ **: Scikit-learn
- **æ¨¡å‹**: {best_model_name}
"""
        
        print("âœ… é¡¹ç›®æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        return report
    
    def run_complete_project(self):
        """è¿è¡Œå®Œæ•´é¡¹ç›®"""
        print("ğŸš¢ å¼€å§‹æ³°å¦å°¼å…‹å·ç”Ÿè¿˜é¢„æµ‹å®Œæ•´é¡¹ç›®...")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. æ¢ç´¢æ€§æ•°æ®åˆ†æ
        self.exploratory_data_analysis()
        
        # 3. å‡†å¤‡æ•°æ®
        self.prepare_data()
        
        # 4. è®­ç»ƒå¤šç§æ¨¡å‹
        self.train_models()
        
        # 5. è¯„ä¼°æ¨¡å‹
        self.evaluate_models()
        
        # 6. è¶…å‚æ•°è°ƒä¼˜
        self.hyperparameter_tuning()
        
        # 7. é‡æ–°è¯„ä¼°è°ƒä¼˜åçš„æ¨¡å‹
        print("\\nğŸ”„ é‡æ–°è¯„ä¼°è°ƒä¼˜åçš„æ¨¡å‹...")
        y_pred_final = self.best_model.predict(self.X_test_scaled)
        y_pred_proba_final = self.best_model.predict_proba(self.X_test_scaled)[:, 1]
        
        final_accuracy = accuracy_score(self.y_test, y_pred_final)
        final_auc = roc_auc_score(self.y_test, y_pred_proba_final)
        
        print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.4f}")
        print(f"   æœ€ç»ˆAUC: {final_auc:.4f}")
        
        # 8. ç”Ÿæˆé¡¹ç›®æŠ¥å‘Š
        report = self.create_project_report()
        
        print("\\nğŸ¯ é¡¹ç›®æ€»ç»“:")
        print(f"  â€¢ æˆåŠŸå®Œæˆç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ é¡¹ç›®")
        print(f"  â€¢ æœ€ä½³æ¨¡å‹å‡†ç¡®ç‡: {final_accuracy:.4f}")
        print(f"  â€¢ æ¨¡å‹AUCåˆ†æ•°: {final_auc:.4f}")
        print(f"  â€¢ ç‰¹å¾å·¥ç¨‹åˆ›å»º {len(self.feature_names)} ä¸ªç‰¹å¾")
        print("\\nğŸ‰ æ³°å¦å°¼å…‹å·ç”Ÿè¿˜é¢„æµ‹é¡¹ç›®å®Œæˆ!")
        
        return self, report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš¢ ç¬¬14ç« ï¼šå®Œæ•´AIé¡¹ç›®å®æˆ˜")
    print("=" * 60)
    
    # è¿è¡Œå®Œæ•´é¡¹ç›®
    predictor, report = TitanicSurvivalPredictor().run_complete_project()
    
    print("\\nğŸ“ å­¦ä¹ æ€»ç»“:")
    print("  â€¢ æŒæ¡äº†å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®æµç¨‹")
    print("  â€¢ å­¦ä¼šäº†æ•°æ®æ¢ç´¢å’Œç‰¹å¾å·¥ç¨‹")
    print("  â€¢ æ¯”è¾ƒäº†å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•")
    print("  â€¢ è¿›è¡Œäº†æ¨¡å‹è°ƒä¼˜å’Œæ€§èƒ½è¯„ä¼°")
    print("  â€¢ å…·å¤‡äº†ç«¯åˆ°ç«¯é¡¹ç›®å¼€å‘èƒ½åŠ›")
    
    return predictor, report

if __name__ == "__main__":
    predictor, report = main()
