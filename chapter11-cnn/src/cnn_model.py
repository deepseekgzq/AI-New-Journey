"""
ç¬¬11ç« ï¼šCNNæ‰‹å†™æ•°å­—è¯†åˆ«å®Œæ•´å®ç°
MNISTæ•°æ®é›†æ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class MNISTCNNClassifier:
    """MNISTæ‰‹å†™æ•°å­—CNNåˆ†ç±»å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.model = None
        self.history = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.y_train_cat = None
        self.y_test_cat = None
        self.class_names = [str(i) for i in range(10)]
        
    def load_data(self):
        """åŠ è½½MNISTæ•°æ®é›†"""
        print("ğŸ“‚ åŠ è½½MNISTæ•°æ®é›†...")
        
        # åŠ è½½æ•°æ®
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
        print(f"   - è®­ç»ƒé›†: {X_train.shape[0]} å¼ å›¾ç‰‡")
        print(f"   - æµ‹è¯•é›†: {X_test.shape[0]} å¼ å›¾ç‰‡")
        print(f"   - å›¾ç‰‡å°ºå¯¸: {X_train.shape[1]}Ã—{X_train.shape[2]}")
        print(f"   - ç±»åˆ«æ•°é‡: {len(np.unique(y_train))}")
        
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        
        return X_train, X_test, y_train, y_test
    
    def explore_data(self):
        """æ•°æ®æ¢ç´¢æ€§åˆ†æ"""
        print("\nğŸ” æ•°æ®æ¢ç´¢åˆ†æ...")
        
        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.X_train.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.X_test.shape}")
        print(f"åƒç´ å€¼èŒƒå›´: {self.X_train.min()} - {self.X_train.max()}")
        
        # ç±»åˆ«åˆ†å¸ƒ
        unique, counts = np.unique(self.y_train, return_counts=True)
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique, counts))}")
        
        # å¯è§†åŒ–æ•°æ®æ ·æœ¬
        fig, axes = plt.subplots(3, 4, figsize=(12, 9))
        
        # æ˜¾ç¤ºæ¯ä¸ªæ•°å­—çš„ç¤ºä¾‹
        for i in range(10):
            if i < 12:  # åªæ˜¾ç¤ºå‰12ä¸ª
                row, col = i // 4, i % 4
                if row < 3:
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¯¥æ•°å­—çš„æ ·æœ¬
                    idx = np.where(self.y_train == i)[0][0]
                    axes[row, col].imshow(self.X_train[idx], cmap='gray')
                    axes[row, col].set_title(f'æ•°å­— {i}', fontweight='bold')
                    axes[row, col].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(10, 12):
            row, col = i // 4, i % 4
            if row < 3:
                axes[row, col].axis('off')
        
        plt.suptitle('MNISTæ•°æ®é›†æ ·æœ¬å±•ç¤º', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # ç±»åˆ«åˆ†å¸ƒæŸ±çŠ¶å›¾
        plt.figure(figsize=(10, 6))
        plt.bar(unique, counts, color='skyblue', edgecolor='black', alpha=0.7)
        plt.title('è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        plt.xlabel('æ•°å­—ç±»åˆ«')
        plt.ylabel('æ ·æœ¬æ•°é‡')
        plt.xticks(unique)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, count in enumerate(counts):
            plt.text(i, count + 50, str(count), ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        print("ğŸ’¡ æ•°æ®ç‰¹ç‚¹:")
        print("  â€¢ å›¾ç‰‡å°ºå¯¸: 28Ã—28åƒç´ ")
        print("  â€¢ ç°åº¦å›¾åƒï¼Œåƒç´ å€¼0-255")
        print("  â€¢ 10ä¸ªç±»åˆ«ï¼Œåˆ†å¸ƒç›¸å¯¹å‡åŒ€")
        print("  â€¢ è®­ç»ƒé›†6ä¸‡å¼ ï¼Œæµ‹è¯•é›†1ä¸‡å¼ ")
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
        
        # å½’ä¸€åŒ–åƒç´ å€¼åˆ°0-1èŒƒå›´
        self.X_train = self.X_train.astype('float32') / 255.0
        self.X_test = self.X_test.astype('float32') / 255.0
        
        # é‡å¡‘æ•°æ®ä¸ºCNNè¾“å…¥æ ¼å¼ (æ ·æœ¬æ•°, é«˜, å®½, é€šé“æ•°)
        self.X_train = self.X_train.reshape(-1, 28, 28, 1)
        self.X_test = self.X_test.reshape(-1, 28, 28, 1)
        
        # å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 
        self.y_train_cat = to_categorical(self.y_train, 10)
        self.y_test_cat = to_categorical(self.y_test, 10)
        
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print(f"   - åƒç´ å€¼å·²å½’ä¸€åŒ–åˆ° [0, 1]")
        print(f"   - è®­ç»ƒæ•°æ®å½¢çŠ¶: {self.X_train.shape}")
        print(f"   - æµ‹è¯•æ•°æ®å½¢çŠ¶: {self.X_test.shape}")
        print(f"   - æ ‡ç­¾å·²è½¬æ¢ä¸ºone-hotç¼–ç ")
        
        return self.X_train, self.X_test, self.y_train_cat, self.y_test_cat
    
    def build_model(self):
        """æ„å»ºCNNæ¨¡å‹"""
        print("\nğŸ—ï¸ æ„å»ºCNNæ¨¡å‹...")
        
        model = models.Sequential([
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            layers.BatchNormalization(),
            layers.Conv2D(32, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # ç¬¬ä¸‰ä¸ªå·ç§¯å—
            layers.Conv2D(128, (3, 3), activation='relu'),
            layers.Dropout(0.25),
            
            # å±•å¹³å’Œå…¨è¿æ¥å±‚
            layers.Flatten(),
            layers.Dense(512, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(10, activation='softmax')
        ])
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        
        print("âœ… CNNæ¨¡å‹æ„å»ºå®Œæˆ!")
        print(f"   - æ€»å‚æ•°æ•°é‡: {model.count_params():,}")
        
        # æ˜¾ç¤ºæ¨¡å‹ç»“æ„
        print("\nğŸ“‹ æ¨¡å‹æ¶æ„:")
        model.summary()
        
        return model
    
    def visualize_model_architecture(self):
        """å¯è§†åŒ–æ¨¡å‹æ¶æ„"""
        print("\nğŸ¨ å¯è§†åŒ–æ¨¡å‹æ¶æ„...")
        
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹è¾“å…¥æ¥å¯è§†åŒ–ç‰¹å¾æ˜ å°„
        sample_input = self.X_train[0:1]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        
        # è·å–æ¯å±‚çš„è¾“å‡º
        layer_outputs = []
        layer_names = []
        
        for layer in self.model.layers:
            if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):
                layer_outputs.append(layer.output)
                layer_names.append(layer.name)
        
        # åˆ›å»ºæ¿€æ´»æ¨¡å‹
        if layer_outputs:
            activation_model = models.Model(inputs=self.model.input, outputs=layer_outputs)
            activations = activation_model.predict(sample_input)
            
            # å¯è§†åŒ–å‰å‡ å±‚çš„ç‰¹å¾æ˜ å°„
            fig, axes = plt.subplots(2, 4, figsize=(16, 8))
            
            for i, (activation, name) in enumerate(zip(activations[:8], layer_names[:8])):
                if i < 8:
                    row, col = i // 4, i % 4
                    
                    if len(activation.shape) == 4:  # å·ç§¯å±‚è¾“å‡º
                        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç‰¹å¾æ˜ å°„
                        feature_map = activation[0, :, :, 0]
                        axes[row, col].imshow(feature_map, cmap='viridis')
                        axes[row, col].set_title(f'{name}\\n{activation.shape[1:]}', fontsize=10)
                        axes[row, col].axis('off')
            
            plt.suptitle('CNNç‰¹å¾æ˜ å°„å¯è§†åŒ–', fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.show()
    
    def train_model(self, epochs=20, batch_size=128, validation_split=0.1):
        """è®­ç»ƒCNNæ¨¡å‹"""
        print(f"\\nğŸ¤– è®­ç»ƒCNNæ¨¡å‹ (epochs={epochs}, batch_size={batch_size})...")
        
        # è®¾ç½®å›è°ƒå‡½æ•°
        callbacks_list = [
            callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=5,
                restore_best_weights=True
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,
                min_lr=1e-7
            )
        ]
        
        # è®­ç»ƒæ¨¡å‹
        self.history = self.model.fit(
            self.X_train, self.y_train_cat,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=callbacks_list,
            verbose=1
        )
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        
        return self.history
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        print("\\nğŸ“ˆ å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹...")
        
        if self.history is None:
            print("âŒ æ²¡æœ‰è®­ç»ƒå†å²è®°å½•")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax1.plot(self.history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        ax1.plot(self.history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡', fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('å‡†ç¡®ç‡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æŸå¤±æ›²çº¿
        ax2.plot(self.history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax2.plot(self.history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
        ax2.set_title('æ¨¡å‹æŸå¤±', fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('æŸå¤±å€¼')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°æœ€ä½³æ€§èƒ½
        best_val_acc = max(self.history.history['val_accuracy'])
        best_epoch = self.history.history['val_accuracy'].index(best_val_acc) + 1
        print(f"ğŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (Epoch {best_epoch})")
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        
        # æµ‹è¯•é›†è¯„ä¼°
        test_loss, test_accuracy = self.model.evaluate(self.X_test, self.y_test_cat, verbose=0)
        print(f"âœ… æµ‹è¯•é›†æ€§èƒ½:")
        print(f"   - å‡†ç¡®ç‡: {test_accuracy:.4f}")
        print(f"   - æŸå¤±å€¼: {test_loss:.4f}")
        
        # é¢„æµ‹
        y_pred_proba = self.model.predict(self.X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # åˆ†ç±»æŠ¥å‘Š
        print("\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(self.y_test, y_pred, target_names=self.class_names))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, y_pred)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=self.class_names, yticklabels=self.class_names)
        plt.title('æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.tight_layout()
        plt.show()
        
        return test_accuracy, test_loss, y_pred, y_pred_proba
    
    def visualize_predictions(self, num_samples=12):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        print(f"\\nğŸ”® å¯è§†åŒ–é¢„æµ‹ç»“æœ (æ˜¾ç¤º{num_samples}ä¸ªæ ·æœ¬)...")
        
        # è·å–é¢„æµ‹
        y_pred_proba = self.model.predict(self.X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # é€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
        indices = np.random.choice(len(self.X_test), num_samples, replace=False)
        
        fig, axes = plt.subplots(3, 4, figsize=(12, 9))
        axes = axes.ravel()
        
        for i, idx in enumerate(indices):
            if i < num_samples:
                # æ˜¾ç¤ºå›¾ç‰‡
                axes[i].imshow(self.X_test[idx].reshape(28, 28), cmap='gray')
                
                # é¢„æµ‹ç»“æœ
                true_label = self.y_test[idx]
                pred_label = y_pred[idx]
                confidence = y_pred_proba[idx][pred_label]
                
                # è®¾ç½®æ ‡é¢˜é¢œè‰²
                color = 'green' if true_label == pred_label else 'red'
                
                axes[i].set_title(f'çœŸå®: {true_label}\\né¢„æµ‹: {pred_label}\\nç½®ä¿¡åº¦: {confidence:.2f}', 
                                color=color, fontweight='bold')
                axes[i].axis('off')
        
        plt.suptitle('é¢„æµ‹ç»“æœå¯è§†åŒ–', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # ç»Ÿè®¡å‡†ç¡®é¢„æµ‹çš„æ•°é‡
        correct_predictions = np.sum(y_pred == self.y_test)
        total_predictions = len(self.y_test)
        accuracy = correct_predictions / total_predictions
        
        print(f"ğŸ“ˆ é¢„æµ‹ç»Ÿè®¡:")
        print(f"   - æ­£ç¡®é¢„æµ‹: {correct_predictions}/{total_predictions}")
        print(f"   - å‡†ç¡®ç‡: {accuracy:.4f}")
    
    def predict_single_image(self, image, show_image=True):
        """é¢„æµ‹å•å¼ å›¾ç‰‡"""
        if image.shape != (28, 28):
            raise ValueError("å›¾ç‰‡å°ºå¯¸å¿…é¡»æ˜¯28x28")
        
        # é¢„å¤„ç†å›¾ç‰‡
        if image.max() > 1.0:
            image = image.astype('float32') / 255.0
        
        image = image.reshape(1, 28, 28, 1)
        
        # é¢„æµ‹
        prediction_proba = self.model.predict(image, verbose=0)
        prediction = np.argmax(prediction_proba)
        confidence = prediction_proba[0][prediction]
        
        if show_image:
            plt.figure(figsize=(6, 4))
            plt.subplot(1, 2, 1)
            plt.imshow(image.reshape(28, 28), cmap='gray')
            plt.title('è¾“å…¥å›¾ç‰‡', fontweight='bold')
            plt.axis('off')
            
            plt.subplot(1, 2, 2)
            plt.bar(range(10), prediction_proba[0])
            plt.title(f'é¢„æµ‹: {prediction} (ç½®ä¿¡åº¦: {confidence:.3f})', fontweight='bold')
            plt.xlabel('æ•°å­—ç±»åˆ«')
            plt.ylabel('æ¦‚ç‡')
            plt.xticks(range(10))
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
        
        return prediction, confidence, prediction_proba[0]
    
    def save_model(self, filepath='mnist_cnn_model.h5'):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is not None:
            self.model.save(filepath)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")
        else:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
    
    def load_model(self, filepath='mnist_cnn_model.h5'):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.model = tf.keras.models.load_model(filepath)
            print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            return self.model
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def run_complete_project(self):
        """è¿è¡Œå®Œæ•´çš„CNNé¡¹ç›®"""
        print("ğŸ–¼ï¸ å¼€å§‹MNISTæ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®...")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. æ•°æ®æ¢ç´¢
        self.explore_data()
        
        # 3. æ•°æ®é¢„å¤„ç†
        self.preprocess_data()
        
        # 4. æ„å»ºæ¨¡å‹
        self.build_model()
        
        # 5. å¯è§†åŒ–æ¨¡å‹æ¶æ„
        self.visualize_model_architecture()
        
        # 6. è®­ç»ƒæ¨¡å‹
        self.train_model(epochs=10)  # å‡å°‘epochsä»¥ä¾¿å¿«é€Ÿæ¼”ç¤º
        
        # 7. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        self.plot_training_history()
        
        # 8. è¯„ä¼°æ¨¡å‹
        test_acc, test_loss, y_pred, y_pred_proba = self.evaluate_model()
        
        # 9. å¯è§†åŒ–é¢„æµ‹ç»“æœ
        self.visualize_predictions()
        
        # 10. å•å¼ å›¾ç‰‡é¢„æµ‹ç¤ºä¾‹
        print("\\nğŸ” å•å¼ å›¾ç‰‡é¢„æµ‹ç¤ºä¾‹:")
        sample_idx = np.random.randint(0, len(self.X_test))
        sample_image = self.X_test[sample_idx].reshape(28, 28)
        prediction, confidence, proba = self.predict_single_image(sample_image)
        
        print(f"çœŸå®æ ‡ç­¾: {self.y_test[sample_idx]}")
        print(f"é¢„æµ‹ç»“æœ: {prediction}")
        print(f"é¢„æµ‹ç½®ä¿¡åº¦: {confidence:.4f}")
        
        # 11. ä¿å­˜æ¨¡å‹
        self.save_model()
        
        print("\\nğŸ¯ é¡¹ç›®æ€»ç»“:")
        print(f"  â€¢ æ•°æ®é›†: MNIST (60,000è®­ç»ƒ + 10,000æµ‹è¯•)")
        print(f"  â€¢ æ¨¡å‹: CNN (å·ç§¯ç¥ç»ç½‘ç»œ)")
        print(f"  â€¢ æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"  â€¢ æ¨¡å‹å‚æ•°: {self.model.count_params():,}")
        print("\\nğŸ‰ MNISTæ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®å®Œæˆ!")
        
        return self

def demonstrate_cnn_concepts():
    """æ¼”ç¤ºCNNæ ¸å¿ƒæ¦‚å¿µ"""
    print("\\n" + "="*60)
    print("ğŸ§  CNNæ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º")
    print("="*60)
    
    # 1. å·ç§¯æ“ä½œæ¼”ç¤º
    print("\\n1ï¸âƒ£ å·ç§¯æ“ä½œæ¼”ç¤º:")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„5x5å›¾åƒ
    image = np.array([
        [0, 0, 1, 0, 0],
        [0, 1, 1, 1, 0],
        [1, 1, 1, 1, 1],
        [0, 1, 1, 1, 0],
        [0, 0, 1, 0, 0]
    ])
    
    # è¾¹ç¼˜æ£€æµ‹å·ç§¯æ ¸
    kernel = np.array([
        [-1, -1, -1],
        [-1,  8, -1],
        [-1, -1, -1]
    ])
    
    # æ‰‹åŠ¨å·ç§¯è®¡ç®—
    def manual_convolution(image, kernel):
        result = np.zeros((3, 3))
        for i in range(3):
            for j in range(3):
                result[i, j] = np.sum(image[i:i+3, j:j+3] * kernel)
        return result
    
    conv_result = manual_convolution(image, kernel)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    
    axes[0].imshow(image, cmap='gray')
    axes[0].set_title('åŸå§‹å›¾åƒ (5Ã—5)', fontweight='bold')
    axes[0].grid(True)
    
    axes[1].imshow(kernel, cmap='RdBu')
    axes[1].set_title('å·ç§¯æ ¸ (3Ã—3)\\nè¾¹ç¼˜æ£€æµ‹', fontweight='bold')
    
    axes[2].imshow(conv_result, cmap='gray')
    axes[2].set_title('å·ç§¯ç»“æœ (3Ã—3)', fontweight='bold')
    
    for ax in axes:
        ax.set_xticks(range(ax.get_xlim()[1]))
        ax.set_yticks(range(ax.get_ylim()[1]))
    
    plt.tight_layout()
    plt.show()
    
    print("   âœ… å·ç§¯æ“ä½œå°†å±€éƒ¨ç‰¹å¾è½¬æ¢ä¸ºç‰¹å¾æ˜ å°„")
    
    # 2. æ± åŒ–æ“ä½œæ¼”ç¤º
    print("\\n2ï¸âƒ£ æ± åŒ–æ“ä½œæ¼”ç¤º:")
    
    # åˆ›å»ºä¸€ä¸ª4x4ç‰¹å¾æ˜ å°„
    feature_map = np.random.randint(0, 10, (4, 4))
    
    # æœ€å¤§æ± åŒ–
    def max_pooling(feature_map, pool_size=2):
        h, w = feature_map.shape
        result = np.zeros((h//pool_size, w//pool_size))
        for i in range(0, h, pool_size):
            for j in range(0, w, pool_size):
                result[i//pool_size, j//pool_size] = np.max(feature_map[i:i+pool_size, j:j+pool_size])
        return result
    
    pooled_result = max_pooling(feature_map)
    
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
    
    im1 = axes[0].imshow(feature_map, cmap='viridis')
    axes[0].set_title('åŸå§‹ç‰¹å¾æ˜ å°„ (4Ã—4)', fontweight='bold')
    plt.colorbar(im1, ax=axes[0])
    
    im2 = axes[1].imshow(pooled_result, cmap='viridis')
    axes[1].set_title('æœ€å¤§æ± åŒ–ç»“æœ (2Ã—2)', fontweight='bold')
    plt.colorbar(im2, ax=axes[1])
    
    # æ·»åŠ ç½‘æ ¼å’Œæ•°å€¼
    for ax, data in zip(axes, [feature_map, pooled_result]):
        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                ax.text(j, i, f'{data[i,j]:.0f}', ha='center', va='center', 
                       color='white', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    print("   âœ… æ± åŒ–æ“ä½œå‡å°‘ç‰¹å¾æ˜ å°„å°ºå¯¸ï¼Œä¿ç•™é‡è¦ä¿¡æ¯")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ–¼ï¸ ç¬¬11ç« ï¼šCNNä¸æ‰‹å†™æ•°å­—è¯†åˆ«")
    print("=" * 60)
    
    # 1. è¿è¡Œå®Œæ•´é¡¹ç›®
    classifier = MNISTCNNClassifier()
    classifier.run_complete_project()
    
    # 2. æ¼”ç¤ºCNNæ¦‚å¿µ
    demonstrate_cnn_concepts()
    
    print("\\nğŸ“ å­¦ä¹ æ€»ç»“:")
    print("  â€¢ CNNæ“…é•¿å¤„ç†å›¾åƒæ•°æ®")
    print("  â€¢ å·ç§¯å±‚æå–å±€éƒ¨ç‰¹å¾")
    print("  â€¢ æ± åŒ–å±‚å‡å°‘è®¡ç®—é‡")
    print("  â€¢ æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æº")
    
    return classifier

if __name__ == "__main__":
    classifier = main()
